{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\Sebastian Vendt\\.julia\\compiled\\v1.1\\FeedbackConvNets\\N4snG.ji for FeedbackConvNets [10ac9144-6928-11e9-2fa1-89aaeb90b0fd]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "include(\"./dataManager.jl\")\n",
    "\n",
    "# Implementation of the Bottom Up Network with two Subversions: \n",
    "# BK with different Kernelsize and BF with different feature size\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehot, onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using LinearAlgebra\n",
    "\n",
    "using MAT # needs installation of Pkg.add(\"MAT\")\n",
    "using PyPlot # pip install Matplotlib Pkg.add(\"PyPlot\")\n",
    "using ..dataManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# ----------\n",
    "batch_size = 100\n",
    "n = 5\n",
    "c = 1\n",
    "alpha = 10^-4\n",
    "beta = 0.5\n",
    "lambda = 0.0005\n",
    "\n",
    "kernelsize = (3, 3)\n",
    "featuresize = 32\n",
    "\n",
    "momentum = 0.9\n",
    "# TODO drop the learning rate according to the paper\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is either 32x32x32xN or 16x16x32xN\n",
    "function localResponseNorm(x)\n",
    "    w = zeros(size(x))\n",
    "    for k = 1:size(x,3)\n",
    "        # calculate the boundaries of the sum\n",
    "        # ATTENTION: this will sum n+1 Elements\n",
    "        lower = Int32(max(1, floor(k-n/2)))\n",
    "        upper = Int32(min(size(x,3), floor(k+n/2)))\n",
    "        \n",
    "        # square and sum \n",
    "        # w = map((x) -> x^2, x[:, :, lower:upper, :])\n",
    "        norm = Tracker.data(x[:, :, lower:upper, :])\n",
    "        norm = norm .^ 2\n",
    "        # reduce to one 32x32x1xN or 16x16x1xN matrix\n",
    "        norm = sum(norm, dims=3)\n",
    "        norm = norm .* alpha\n",
    "        norm = norm .+ c\n",
    "        norm = norm .^ (-beta)\n",
    "        w[:, :, k, :] = norm\n",
    "    end   \n",
    "    # a tracked array multiplied with a non tracked array returns a tracked array  \n",
    "    x = w .* x\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, y)\n",
    "    y_hat = model(x)\n",
    "    return crossentropy(y_hat, y) + lambda * sum(norm, params(model))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info(\"Constructing model...\")\n",
    "model = Chain(\n",
    "    # HIDDEN LAYER 1\n",
    "    # Input Image 32x32x1xN\n",
    "    Conv(kernelsize, 1=>featuresize, pad=(1,1), relu),\n",
    "    # local response normalization\n",
    "    x -> localResponseNorm(x),\n",
    "    MaxPool((2, 2), stride=(2, 2)),\n",
    "    \n",
    "    # HIDDEN LAYER 2\n",
    "    # Input Image 16x16x32xN\n",
    "    Conv(kernelsize, featuresize=>featuresize, pad=(1,1), relu),\n",
    "    # local response normalization\n",
    "    x -> localResponseNorm(x),\n",
    "    MaxPool((16, 16), stride=(1, 1)),\n",
    "    # reshape to 32xN\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(32, 10, σ),\n",
    ")\n",
    "\n",
    "# test the model (precomile it??)\n",
    "model(rand(32, 32, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training...\n",
      "└ @ Main In[4]:1\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: learningRate not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: learningRate not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[4]:2"
     ]
    }
   ],
   "source": [
    "@info(\"Training...\")\n",
    "optimizer = Momentum(learningRate, momentum)\n",
    "# using the momentum update rule\n",
    "\n",
    "label = zeros(10, 2)\n",
    "label[3, 1] = 1\n",
    "label[4, 2] = 1\n",
    "\n",
    "data = [(rand(32,32,1,2), label), (rand(32,32,1,2), label)]\n",
    "# training probably works but pixelwise normalization is missing \n",
    "# so loss gets NaN\n",
    "Flux.train!(loss, params(model), data, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
