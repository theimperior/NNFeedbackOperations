{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./dataManager.jl\")\n",
    "\n",
    "# Implementation of the Bottom Up Network with two Subversions: \n",
    "# BK with different Kernelsize and BF with different feature size\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehot, onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using LinearAlgebra\n",
    "\n",
    "using MAT # needs installation of Pkg.add(\"MAT\")\n",
    "using PyPlot # pip install Matplotlib Pkg.add(\"PyPlot\")\n",
    "using ..dataManager: make_batch\n",
    "using FeedbackConvNets\n",
    "using Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "# ----------\n",
    "batch_size = 100\n",
    "n = 5\n",
    "c = 1\n",
    "alpha = Float32(10^-4)\n",
    "beta = 0.5f0\n",
    "lambda = 0.0005f0\n",
    "\n",
    "kernelsize = (3, 3)\n",
    "featuresize = 32\n",
    "\n",
    "momentum = 0.9f0\n",
    "learningRate = 0.01f0\n",
    "epochs = 100\n",
    "decay_rate = 0.1f0\n",
    "decay_step = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_callback (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input is either 32x32x32xN or 16x16x32xN\n",
    "function localResponseNorm(x)\n",
    "    w = zeros(size(x))\n",
    "    for k = 1:size(x,3)\n",
    "        # calculate the boundaries of the sum\n",
    "        # ATTENTION: this will sum n+1 Elements\n",
    "        lower = Int32(max(1, floor(k-n/2)))\n",
    "        upper = Int32(min(size(x,3), floor(k+n/2)))\n",
    "        \n",
    "        # square and sum \n",
    "        # w = map((x) -> x^2, x[:, :, lower:upper, :])\n",
    "        norm = Tracker.data(x[:, :, lower:upper, :])\n",
    "        norm = norm .^ 2\n",
    "        # reduce to one 32x32x1xN or 16x16x1xN matrix\n",
    "        norm = sum(norm, dims=3)\n",
    "        norm = norm .* alpha\n",
    "        norm = norm .+ c\n",
    "        norm = norm .^ (-beta)\n",
    "        w[:, :, k, :] = norm\n",
    "    end   \n",
    "    # a tracked array multiplied with a non tracked array returns a tracked array  \n",
    "    x = w .* x\n",
    "    return x\n",
    "end\n",
    "\n",
    "function loss(x, y)\n",
    "    #y_hat = model(x)\n",
    "    #cr = crossentropy(y_hat, y) \n",
    "    #sum2 = lambda * sum(norm, params(model))\n",
    "    #display(params(model))\n",
    "    #return cr + sum2\n",
    "    y_hat = model(x)\n",
    "    cr = crossentropy(y_hat, y) \n",
    "    sum2 = sum(norm, params(model))\n",
    "    display(params(model))\n",
    "    display(typeof(sum2))\n",
    "    return cr + sum2\n",
    "end\n",
    "\n",
    "# loss = loss |> gpu\n",
    "\n",
    "training = 0\n",
    "function training_callback()\n",
    "    training = training + 1\n",
    "    @info(\"now at batch $(training)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Constructing model...\n",
      "└ @ Main In[4]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tracked 10×1 Array{Float32,2}:\n",
       " 0.54341644f0\n",
       " 0.6481357f0 \n",
       " 0.23100382f0\n",
       " 0.33359793f0\n",
       " 0.5924079f0 \n",
       " 0.22047447f0\n",
       " 0.7265448f0 \n",
       " 0.79020816f0\n",
       " 0.5094831f0 \n",
       " 0.5608709f0 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info(\"Constructing model...\")\n",
    "model = Chain(\n",
    "    # HIDDEN LAYER 1\n",
    "    # Input Image 32x32x1xN\n",
    "    Conv(kernelsize, 1=>featuresize, pad=(1,1), relu),\n",
    "    # local response normalization\n",
    "    x -> localResponseNorm(x),\n",
    "    MaxPool((2, 2), stride=(2, 2)),\n",
    "    \n",
    "    # HIDDEN LAYER 2\n",
    "    # Input Image 16x16x32xN\n",
    "    Conv(kernelsize, featuresize=>featuresize, pad=(1,1), relu),\n",
    "    # local response normalization\n",
    "    x -> localResponseNorm(x),\n",
    "    MaxPool((16, 16), stride=(1, 1)),\n",
    "    # reshape to 32xN\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(32, 10, σ),\n",
    ")\n",
    "\n",
    "# test the model (precomile it??)\n",
    "#model = model |> gpu\n",
    "model(rand(32, 32, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading .mat file form source ../digitclutter/src/light_debris/light_debris_with_debris.mat\n",
      "└ @ Main.dataManager /home/sebastian/GIT/NNFeedbackOperations/Src/dataManager.jl:39\n",
      "┌ Info: calculate mean and standart deviation of dataset\n",
      "└ @ Main.dataManager /home/sebastian/GIT/NNFeedbackOperations/Src/dataManager.jl:55\n",
      "┌ Info: creating batches\n",
      "└ @ Main.dataManager /home/sebastian/GIT/NNFeedbackOperations/Src/dataManager.jl:73\n",
      "┌ Info: Training...\n",
      "└ @ Main In[5]:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.341443 0.0265776 -0.369339; 0.382361 0.107868 0.0192445; 0.100131 -0.0275369 -0.300151]\n",
       "\n",
       "Float32[0.231456 -0.050507 0.386814; 0.215755 0.0442183 0.00394429; 0.149338 0.00924802 0.331916]\n",
       "\n",
       "Float32[0.287478 0.103428 0.0771892; 0.208912 0.340392 -0.0410792; 0.137974 -0.237877 0.30995]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.0598139 -0.334501 -0.344253; 0.207081 0.226588 0.021283; 0.355205 0.0666841 0.316642]\n",
       "\n",
       "Float32[-0.256002 0.0181124 0.134672; -0.127226 0.334112 0.226276; -0.0490724 -0.0804722 0.182184]\n",
       "\n",
       "Float32[-0.00844192 0.368635 -0.300859; 0.116597 -0.155978 0.0875092; -0.143139 0.298988 -0.0474887] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked), Float32[0.170274 0.0916203 -0.161485; 0.121573 -0.241848 -0.122713; 0.0773857 -0.187573 0.052955]\n",
       "\n",
       "Float32[-0.191078 -0.238403 -0.220442; -0.0350263 -0.13887 0.0326772; -0.116944 0.276322 0.261597]\n",
       "\n",
       "Float32[0.197514 -0.291552 -0.239531; 0.076442 0.161019 0.0360299; -0.0621127 -0.0389828 -0.0482686]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.0701423 -0.197104 0.208734; 0.0934898 -0.266656 0.174394; -0.0901954 0.0296273 -0.251256]\n",
       "\n",
       "Float32[0.102009 0.0979953 -0.125764; 0.107872 -0.227196 -0.0860164; 0.268626 -0.0591915 -0.183115]\n",
       "\n",
       "Float32[0.0439328 0.234802 0.217452; -0.27675 -0.0622715 0.185966; 0.121678 0.280934 0.0683543]\n",
       "\n",
       "Float32[-0.041304 0.0721481 -0.244996; 0.25007 0.166789 -0.223258; 0.263618 -0.188658 0.00977721]\n",
       "\n",
       "Float32[-0.212861 0.241556 0.247527; 0.0757077 0.227275 0.161884; -0.227413 -0.163983 0.0433322]\n",
       "\n",
       "Float32[-0.110916 -0.26406 -0.269174; 0.145854 -0.117974 0.0300672; -0.144354 0.10068 0.0228835]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.10359 0.219023 0.176383; -0.105377 0.266051 -0.200824; 0.02639 0.26743 -0.279874]\n",
       "\n",
       "Float32[0.0793136 0.0845204 -0.104612; -0.105606 -0.154226 -0.124008; 0.150423 0.207043 -0.109674]\n",
       "\n",
       "Float32[-0.0541795 0.0780593 -0.137862; -0.135084 -0.0878494 -0.00899508; -0.0172989 0.210304 -0.0350538]\n",
       "\n",
       "Float32[-0.278751 -0.155257 -0.0273692; 0.129769 0.113996 0.110458; 0.239147 -0.160231 0.289453]\n",
       "\n",
       "Float32[0.00298493 0.24792 -0.141639; -0.153455 -0.239435 0.28606; 0.144779 0.162001 0.125546]\n",
       "\n",
       "Float32[0.123854 0.0134484 0.263313; 0.282849 0.103588 -0.21003; -0.167827 0.1096 -0.0150825]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.142171 -0.251564 -0.0969417; 0.185887 -0.0733439 -0.0450828; -0.242724 -0.285603 0.23715]\n",
       "\n",
       "Float32[-0.139525 -0.167844 -0.206829; -0.100738 0.0911839 0.00874728; 0.19086 -0.0753317 -0.140534]\n",
       "\n",
       "Float32[0.287336 0.216708 0.216594; -0.173274 -0.201129 -0.0143399; -0.190815 -0.18224 0.00796606]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.178407 0.201236 -0.0930857; 0.224406 -0.11852 -0.0381714; -0.262617 0.191317 0.223281]\n",
       "\n",
       "Float32[0.262471 0.135152 0.167758; -0.275826 0.115114 0.0626757; 0.208532 -0.0580875 0.0113769]\n",
       "\n",
       "Float32[0.248297 0.0227557 -0.0317059; 0.117817 -0.126195 -0.270635; -0.0735646 0.111517 0.192329]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.240661 -0.264835 -0.0869829; 0.255506 -0.0278405 -0.0807532; -0.256437 -0.225101 -0.278349]\n",
       "\n",
       "Float32[-0.200419 -0.0107659 0.0237952; 0.0523962 -0.238961 -0.239056; -0.162025 -0.156476 -0.00189233]\n",
       "\n",
       "Float32[-0.130772 0.00073271 -0.25792; 0.134045 0.0742338 0.107242; 0.21757 -0.0750629 -0.141246]\n",
       "\n",
       "Float32[-0.064692 0.027952 -0.267048; 0.167543 0.273899 0.129376; 0.122719 0.0283037 -0.22345]\n",
       "\n",
       "Float32[0.223616 0.287443 0.225333; 0.291064 0.103933 0.272631; 0.224784 0.158399 0.22947]\n",
       "\n",
       "Float32[-0.141573 0.244608 -0.0712606; 0.196423 0.276962 -0.216702; -0.018513 0.0119489 0.267541]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.270667 0.170347 -0.055737; 0.0383571 0.0941279 0.0293879; 0.0317661 0.244106 0.28875]\n",
       "\n",
       "Float32[0.139325 -0.247208 0.153524; 0.291993 0.0604061 0.211697; 0.0749096 -0.163198 -0.0640774]\n",
       "\n",
       "Float32[-0.0878612 -0.257684 -0.181672; -0.153964 -0.139964 -0.109648; -0.160838 -0.116802 -0.248739]\n",
       "\n",
       "Float32[-0.0382056 -0.157673 0.215972; -0.0230872 0.0822739 0.290862; 0.0108063 0.0610134 0.232135]\n",
       "\n",
       "Float32[0.27108 -0.159019 0.199083; 0.256769 0.0667869 0.134178; 0.124295 -0.120298 0.0795695]\n",
       "\n",
       "Float32[0.204139 -0.144597 -0.0214193; -0.17814 0.0615661 0.0389833; 0.194837 -0.11274 0.152355]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.193492 -0.154588 0.182019; 0.0907994 0.232232 0.173869; 0.0297886 -0.148891 -0.239053]\n",
       "\n",
       "Float32[-0.0652031 0.180089 0.160088; 0.254011 -0.229585 -0.0361183; 0.0895506 -0.173511 0.198824]\n",
       "\n",
       "Float32[0.251043 -0.187952 0.185926; -0.123959 0.164781 -0.277412; 0.163882 -0.0481392 -0.250067] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked), Float32[0.297729 -0.0376773 … -0.000811384 -0.356049; 0.201212 0.176048 … 0.0921036 -0.329663; … ; -0.142351 -0.00382226 … -0.183274 0.351373; 0.159924 -0.230147 … -0.339575 0.192106] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tracker.TrackedReal{Float64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "25.23026352141214 (tracked)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dataset\n",
    "train_set, mean_img, std_img = make_batch(batch_size=10)\n",
    "\n",
    "@info(\"Training...\")\n",
    "optimizer = Momentum(learningRate, momentum)\n",
    "# using the momentum update rule\n",
    "\n",
    "# test_set = gpu.(test_set)\n",
    "\n",
    "\n",
    "train_set = gpu.(train_set)\n",
    "\n",
    "label = zeros(10, 2)\n",
    "label[3, 1] = 1\n",
    "label[4, 2] = 1\n",
    "data = [(rand(32,32,1,2), label), (rand(32,32,1,2), label)]\n",
    "data = gpu.(data) # doesn't convert to float32!!\n",
    "\n",
    "# display(train_set[1][1])\n",
    "# display(mean_img)\n",
    "# display(std_img)\n",
    "# training loop\n",
    "\n",
    "losst = loss(train_set[1][1], train_set[1][2])\n",
    "#display(typeof(losst))\n",
    "\n",
    "# Flux.train!(loss, params(model), train_set, optimizer, cb = training_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
