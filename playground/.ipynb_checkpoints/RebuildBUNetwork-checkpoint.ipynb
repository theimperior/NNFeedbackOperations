{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data set\n",
      "└ @ Main In[1]:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(train_imgs) = (60000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Constructing model...\n",
      "└ @ Main In[1]:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1, 20)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[1]:92\n",
      "┌ Info: [1]: Test accuracy: 0.0980\n",
      "└ @ Main In[1]:102\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:112\n"
     ]
    }
   ],
   "source": [
    "# Classifies MNIST digits with a convolutional network.\n",
    "# Writes out saved model to the file \"mnist_conv.bson\".\n",
    "# Demonstrates basic model construction, training, saving,\n",
    "# conditional early-exit, and learning rate scheduling.\n",
    "#\n",
    "# This model, while simple, should hit around 99% test\n",
    "# accuracy after training for approximately 20 epochs.\n",
    "\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using NNlib\n",
    "#using CuArrays\n",
    "\n",
    "# Load labels and images from Flux.Data.MNIST\n",
    "@info(\"Loading data set\")\n",
    "train_labels = MNIST.labels()\n",
    "train_imgs = MNIST.images()\n",
    "\n",
    "@show(size(train_imgs))\n",
    "\n",
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    # ... expands the inputarguments of the array construction to all sizes of X: size(X[1]), size(X[2]), size(X[3])\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9) \n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "batch_size = 20\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n",
    "\n",
    "\n",
    "# Prepare test set as one giant minibatch:\n",
    "test_imgs = MNIST.images(:test)\n",
    "test_labels = MNIST.labels(:test)\n",
    "test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs))\n",
    "\n",
    "# Define our model.  We will use a simple convolutional architecture with\n",
    "# three iterations of Conv -> ReLU -> MaxPool, followed by a final Dense\n",
    "# layer that feeds into a softmax probability output.\n",
    "@info(\"Constructing model...\")\n",
    "model = Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)\n",
    "    # which is where we get the 288 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10, σ),\n",
    "\n",
    "    # Finally, softmax to get nice probabilities\n",
    ")\n",
    "\n",
    "# Load model and datasets onto GPU, if enabled\n",
    "train_set = gpu.(train_set)\n",
    "test_set = gpu.(test_set)\n",
    "model = model |> gpu\n",
    "\n",
    "# Make sure our model is nicely precompiled before starting our training loop\n",
    "print(size(train_set[1][1]))\n",
    "model(train_set[1][1])\n",
    "\n",
    "# `loss()` calculates the crossentropy loss between our prediction `y_hat`\n",
    "# (calculated from `model(x)`) and the ground truth `y`.  We augment the data\n",
    "# a bit, adding gaussian random noise to our image to make it more robust.\n",
    "function loss(x, y)\n",
    "    # We augment `x` a little bit here, adding in random noise\n",
    "    x_aug = Float32.(x .+ 0.1*randn(eltype(x), size(x)))\n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "# Train our model with the given training set using the ADAM optimizer and\n",
    "# printing out performance against the test set as we go.\n",
    "opt = Momentum(0.01f0, 0.9)\n",
    "\n",
    "@info(\"Beginning training loop...\")\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "for epoch_idx in 1:100\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    Flux.train!(loss, params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = accuracy(test_set...)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "    \n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    # If this is the best accuracy we've seen so far, save the model out\n",
    "    if acc >= best_acc\n",
    "        @info(\" -> New best accuracy! Saving model out to mnist_conv.bson\")\n",
    "        BSON.@save \"mnist_conv.bson\" model epoch_idx acc\n",
    "        best_acc = acc\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "\n",
    "        # After dropping learning rate, give it a few epochs to improve\n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(test_set[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
